{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = r'E:\\20_newsgroups'\n",
    "folders = os.listdir(folder_path) #gives list of all the folders in this path\n",
    "\n",
    "data = {}  #stores folder names as keys and documents as values i.e{folder1:[doc1,doc2..] , folder2:[doc1,doc2..]}\n",
    "for folder in folders:\n",
    "    data[folder] = []\n",
    "    for doc in os.listdir(os.path.join(folder_path,folder)):\n",
    "        opened_doc = open(os.path.join(folder_path,folder,doc),'r') #not exactly storing name of document but contents of doc\n",
    "        data[folder].append(opened_doc.read())\n",
    "\n",
    "from nltk.corpus import stopwords #importing stopwords from nltk\n",
    "from string import punctuation #importing punctuation\n",
    "stopword = stopwords.words('english')\n",
    "stopword += punctuation # list of both nltk stopwords and punctuation\n",
    "\n",
    "import re\n",
    "def ispure(string): #this function checks if word contains any number or special character which is of no use\n",
    "    check = re.compile('[@_.!#$%^&*()<>-?/\\|}{~:1234567890]')\n",
    "    if check.search(string) == None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "freq_map = {} # storing frequency of each word as values and word as keys\n",
    "for i in range(len(data)):\n",
    "    for doc in data[folders[i]]:\n",
    "        for word in doc.split():\n",
    "            if word.lower() not in stopword and len(word)>=5 and ispure(word):\n",
    "                freq_map[word.lower()] = freq_map.get(word.lower(),0)+1\n",
    "                \n",
    "import operator\n",
    "sorted_freq_map = sorted(freq_map.items(),key=operator.itemgetter(1),reverse=True) # reverse sorting words on the basis of frequency \n",
    "features = [key[0] for key in sorted_freq_map] #picking only words from tuple of words and freq returned from above sorted function\n",
    "feature_list = features[0:2500] # picking top 2500 words\n",
    "\n",
    "Y = []\n",
    "for i in range(len(data)): #creating class labels\n",
    "    for doc in data[folders[i]]:\n",
    "        Y.append(folders[i])\n",
    "Y = np.array(Y)\n",
    "\n",
    "matrix = np.zeros(shape=(len(Y),len(feature_list))) #making a matrix of required size to be converted to X\n",
    "df = pd.DataFrame(data=matrix,columns=feature_list) #converting np array to dataframe\n",
    "\n",
    "i=0 #iterator to access each row of dataframe\n",
    "for folder in data: # updating frequency of each word in each document represented by rows in dataframe\n",
    "    for doc in data[folder]:\n",
    "        for word in doc.split():\n",
    "            if word.lower() in feature_list:\n",
    "                df[word.lower()][i] += 1\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive bayes classifier implementation\n",
    "class NBclassifier:\n",
    "    def __init__(self):\n",
    "        self.__result = {} #stores final dictionary with all the counts\n",
    "        \n",
    "    def fit(self,x,y):\n",
    "        feature_names = x.columns #getting the feature names\n",
    "        x = x.values #converting dataframe to np array\n",
    "        self.__result['total_data'] = len(y) #storing total datapoints\n",
    "        class_values = set(y) #getting unique classes\n",
    "        for current_class in class_values: #for each class\n",
    "            self.__result[current_class] = {} #another dictionary against each class\n",
    "            curr_class = (y==current_class)\n",
    "            x_curr_class = x[curr_class] \n",
    "            y_curr_class = y[curr_class]\n",
    "            total_words = 0\n",
    "            for i in range(len(feature_names)): #for each feature\n",
    "                curr_sum = x_curr_class[:,i].sum() #sum of frequency of word for each class\n",
    "                self.__result[current_class][feature_names[i]] = curr_sum\n",
    "                total_words += curr_sum\n",
    "            self.__result[current_class]['total_count'] = total_words\n",
    "        return self.__result\n",
    "    \n",
    "    def predict(self,x):\n",
    "        y_pred = []\n",
    "        feature_names = x.columns\n",
    "        x = x.values #converting dataframe to np array\n",
    "        for row in x: #for each row\n",
    "            x_class = self.__predict_single_class(row,feature_names)\n",
    "            y_pred.append(x_class)\n",
    "        return y_pred\n",
    "    \n",
    "    def __predict_single_class(self,row,feature_names):\n",
    "        first_run = True \n",
    "        best_p = -np.inf\n",
    "        best_class = -np.inf\n",
    "        classes = self.__result.keys() \n",
    "        for curr_class in classes:\n",
    "            if curr_class == 'total_data': #  as dictionary has one extra key storing total data points\n",
    "                continue\n",
    "            p_curr_class = self.__probability(row,curr_class,feature_names)\n",
    "            if first_run or p_curr_class > best_p: #getting most probable class, document(row) belongs to\n",
    "                best_p = p_curr_class\n",
    "                best_class = curr_class\n",
    "            first_run = False\n",
    "        return best_class\n",
    "    \n",
    "    def __probability(self,row,curr_class,feature_names):\n",
    "        output = np.log(self.__result[curr_class]['total_count']) - np.log(self.__result['total_data'])\n",
    "        for i in range(len(feature_names)): #for each feature\n",
    "            curr_count = self.__result[curr_class][feature_names[i]] + 1 #laplace correction\n",
    "            total_count = self.__result[curr_class]['total_count'] + len(feature_names) #laplace correction\n",
    "            curr_probability = np.log(curr_count) - np.log(total_count)\n",
    "            for j in range(int(row[i])): # ignoring the word where frequency is 0 for any particular document\n",
    "                output += curr_probability\n",
    "        return output\n",
    "    \n",
    "    def score(self,x,y): #calculates mean accuracy\n",
    "        y_pred = self.predict(x)\n",
    "        count = 0\n",
    "        for i in range(len(y)):\n",
    "            if y_pred[i] == y[i]:\n",
    "                count += 1\n",
    "        return count/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn score =  0.7038\n",
      "-----------------------------------------------\n",
      "\n",
      "Our score =  0.703\n",
      "------------------------------------------------\n",
      "Sklearn classification report\n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.72      0.59      0.65       314\n",
      "           comp.graphics       0.59      0.61      0.60       241\n",
      " comp.os.ms-windows.misc       0.74      0.58      0.65       293\n",
      "comp.sys.ibm.pc.hardware       0.65      0.53      0.58       279\n",
      "   comp.sys.mac.hardware       0.61      0.65      0.63       227\n",
      "          comp.windows.x       0.65      0.78      0.71       203\n",
      "            misc.forsale       0.75      0.69      0.72       281\n",
      "               rec.autos       0.72      0.68      0.70       260\n",
      "         rec.motorcycles       0.75      0.67      0.71       263\n",
      "      rec.sport.baseball       0.76      0.83      0.80       243\n",
      "        rec.sport.hockey       0.87      0.88      0.87       229\n",
      "               sci.crypt       0.86      0.91      0.88       243\n",
      "         sci.electronics       0.66      0.69      0.68       254\n",
      "                 sci.med       0.66      0.82      0.73       209\n",
      "               sci.space       0.78      0.82      0.80       240\n",
      "  soc.religion.christian       0.78      0.80      0.79       241\n",
      "      talk.politics.guns       0.79      0.64      0.71       304\n",
      "   talk.politics.mideast       0.84      0.88      0.86       256\n",
      "      talk.politics.misc       0.53      0.59      0.56       227\n",
      "      talk.religion.misc       0.38      0.51      0.43       193\n",
      "\n",
      "                accuracy                           0.70      5000\n",
      "               macro avg       0.70      0.71      0.70      5000\n",
      "            weighted avg       0.71      0.70      0.70      5000\n",
      "\n",
      "------------------------------------------------\n",
      "Our classification report\n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.72      0.59      0.65       315\n",
      "           comp.graphics       0.60      0.58      0.59       255\n",
      " comp.os.ms-windows.misc       0.74      0.60      0.66       282\n",
      "comp.sys.ibm.pc.hardware       0.65      0.53      0.59       275\n",
      "   comp.sys.mac.hardware       0.62      0.65      0.63       231\n",
      "          comp.windows.x       0.65      0.78      0.71       203\n",
      "            misc.forsale       0.74      0.72      0.73       267\n",
      "               rec.autos       0.72      0.69      0.70       255\n",
      "         rec.motorcycles       0.72      0.70      0.71       241\n",
      "      rec.sport.baseball       0.76      0.85      0.80       237\n",
      "        rec.sport.hockey       0.86      0.87      0.86       230\n",
      "               sci.crypt       0.86      0.88      0.87       251\n",
      "         sci.electronics       0.64      0.70      0.67       243\n",
      "                 sci.med       0.67      0.82      0.73       213\n",
      "               sci.space       0.78      0.82      0.80       245\n",
      "  soc.religion.christian       0.80      0.79      0.79       251\n",
      "      talk.politics.guns       0.80      0.64      0.71       305\n",
      "   talk.politics.mideast       0.84      0.83      0.84       270\n",
      "      talk.politics.misc       0.54      0.57      0.56       240\n",
      "      talk.religion.misc       0.37      0.50      0.42       191\n",
      "\n",
      "                accuracy                           0.70      5000\n",
      "               macro avg       0.70      0.71      0.70      5000\n",
      "            weighted avg       0.71      0.70      0.70      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing and comparing with sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(df,Y)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred_sk = clf.predict(x_test)\n",
    "sklearn_score = clf.score(x_test,y_test)\n",
    "print('Sklearn score = ',sklearn_score)\n",
    "print('-----------------------------------------------')\n",
    "\n",
    "clf2 = NBclassifier()\n",
    "clf2.fit(x_train,y_train)\n",
    "y_pred_our = clf2.predict(x_test)\n",
    "our_score = clf2.score(x_test,y_test)\n",
    "print()\n",
    "print('Our score = ',our_score)\n",
    "print('------------------------------------------------')\n",
    "from sklearn.metrics import classification_report\n",
    "print('Sklearn classification report')\n",
    "print()\n",
    "print(classification_report(y_pred_sk,y_test))\n",
    "print('------------------------------------------------')\n",
    "print('Our classification report')\n",
    "print()\n",
    "print(classification_report(y_pred_our,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
